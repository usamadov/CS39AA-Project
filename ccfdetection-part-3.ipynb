{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7029086,"sourceType":"datasetVersion","datasetId":4042878}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Loading the imbalanced dataset","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import InputLayer, Dense, BatchNormalization\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom sklearn.metrics import classification_report\n\n\ndf = pd.read_csv('/kaggle/input/ccfrauddetection/creditcard.csv')\ndf","metadata":{"execution":{"iopub.status.busy":"2023-12-01T20:58:27.823496Z","iopub.execute_input":"2023-12-01T20:58:27.823993Z","iopub.status.idle":"2023-12-01T20:58:55.131157Z","shell.execute_reply.started":"2023-12-01T20:58:27.823952Z","shell.execute_reply":"2023-12-01T20:58:55.129559Z"},"trusted":true},"execution_count":1,"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"            Time         V1         V2        V3        V4        V5  \\\n0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n...          ...        ...        ...       ...       ...       ...   \n284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n\n              V6        V7        V8        V9  ...       V21       V22  \\\n0       0.462388  0.239599  0.098698  0.363787  ... -0.018307  0.277838   \n1      -0.082361 -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672   \n2       1.800499  0.791461  0.247676 -1.514654  ...  0.247998  0.771679   \n3       1.247203  0.237609  0.377436 -1.387024  ... -0.108300  0.005274   \n4       0.095921  0.592941 -0.270533  0.817739  ... -0.009431  0.798278   \n...          ...       ...       ...       ...  ...       ...       ...   \n284802 -2.606837 -4.918215  7.305334  1.914428  ...  0.213454  0.111864   \n284803  1.058415  0.024330  0.294869  0.584800  ...  0.214205  0.924384   \n284804  3.031260 -0.296827  0.708417  0.432454  ...  0.232045  0.578229   \n284805  0.623708 -0.686180  0.679145  0.392087  ...  0.265245  0.800049   \n284806 -0.649617  1.577006 -0.414650  0.486180  ...  0.261057  0.643078   \n\n             V23       V24       V25       V26       V27       V28  Amount  \\\n0      -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62   \n1       0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69   \n2       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66   \n3      -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50   \n4      -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99   \n...          ...       ...       ...       ...       ...       ...     ...   \n284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77   \n284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79   \n284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88   \n284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   10.00   \n284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00   \n\n        Class  \n0           0  \n1           0  \n2           0  \n3           0  \n4           0  \n...       ...  \n284802      0  \n284803      0  \n284804      0  \n284805      0  \n284806      0  \n\n[284807 rows x 31 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>-1.359807</td>\n      <td>-0.072781</td>\n      <td>2.536347</td>\n      <td>1.378155</td>\n      <td>-0.338321</td>\n      <td>0.462388</td>\n      <td>0.239599</td>\n      <td>0.098698</td>\n      <td>0.363787</td>\n      <td>...</td>\n      <td>-0.018307</td>\n      <td>0.277838</td>\n      <td>-0.110474</td>\n      <td>0.066928</td>\n      <td>0.128539</td>\n      <td>-0.189115</td>\n      <td>0.133558</td>\n      <td>-0.021053</td>\n      <td>149.62</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>1.191857</td>\n      <td>0.266151</td>\n      <td>0.166480</td>\n      <td>0.448154</td>\n      <td>0.060018</td>\n      <td>-0.082361</td>\n      <td>-0.078803</td>\n      <td>0.085102</td>\n      <td>-0.255425</td>\n      <td>...</td>\n      <td>-0.225775</td>\n      <td>-0.638672</td>\n      <td>0.101288</td>\n      <td>-0.339846</td>\n      <td>0.167170</td>\n      <td>0.125895</td>\n      <td>-0.008983</td>\n      <td>0.014724</td>\n      <td>2.69</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>-1.358354</td>\n      <td>-1.340163</td>\n      <td>1.773209</td>\n      <td>0.379780</td>\n      <td>-0.503198</td>\n      <td>1.800499</td>\n      <td>0.791461</td>\n      <td>0.247676</td>\n      <td>-1.514654</td>\n      <td>...</td>\n      <td>0.247998</td>\n      <td>0.771679</td>\n      <td>0.909412</td>\n      <td>-0.689281</td>\n      <td>-0.327642</td>\n      <td>-0.139097</td>\n      <td>-0.055353</td>\n      <td>-0.059752</td>\n      <td>378.66</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>-0.966272</td>\n      <td>-0.185226</td>\n      <td>1.792993</td>\n      <td>-0.863291</td>\n      <td>-0.010309</td>\n      <td>1.247203</td>\n      <td>0.237609</td>\n      <td>0.377436</td>\n      <td>-1.387024</td>\n      <td>...</td>\n      <td>-0.108300</td>\n      <td>0.005274</td>\n      <td>-0.190321</td>\n      <td>-1.175575</td>\n      <td>0.647376</td>\n      <td>-0.221929</td>\n      <td>0.062723</td>\n      <td>0.061458</td>\n      <td>123.50</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.0</td>\n      <td>-1.158233</td>\n      <td>0.877737</td>\n      <td>1.548718</td>\n      <td>0.403034</td>\n      <td>-0.407193</td>\n      <td>0.095921</td>\n      <td>0.592941</td>\n      <td>-0.270533</td>\n      <td>0.817739</td>\n      <td>...</td>\n      <td>-0.009431</td>\n      <td>0.798278</td>\n      <td>-0.137458</td>\n      <td>0.141267</td>\n      <td>-0.206010</td>\n      <td>0.502292</td>\n      <td>0.219422</td>\n      <td>0.215153</td>\n      <td>69.99</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>284802</th>\n      <td>172786.0</td>\n      <td>-11.881118</td>\n      <td>10.071785</td>\n      <td>-9.834783</td>\n      <td>-2.066656</td>\n      <td>-5.364473</td>\n      <td>-2.606837</td>\n      <td>-4.918215</td>\n      <td>7.305334</td>\n      <td>1.914428</td>\n      <td>...</td>\n      <td>0.213454</td>\n      <td>0.111864</td>\n      <td>1.014480</td>\n      <td>-0.509348</td>\n      <td>1.436807</td>\n      <td>0.250034</td>\n      <td>0.943651</td>\n      <td>0.823731</td>\n      <td>0.77</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>284803</th>\n      <td>172787.0</td>\n      <td>-0.732789</td>\n      <td>-0.055080</td>\n      <td>2.035030</td>\n      <td>-0.738589</td>\n      <td>0.868229</td>\n      <td>1.058415</td>\n      <td>0.024330</td>\n      <td>0.294869</td>\n      <td>0.584800</td>\n      <td>...</td>\n      <td>0.214205</td>\n      <td>0.924384</td>\n      <td>0.012463</td>\n      <td>-1.016226</td>\n      <td>-0.606624</td>\n      <td>-0.395255</td>\n      <td>0.068472</td>\n      <td>-0.053527</td>\n      <td>24.79</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>284804</th>\n      <td>172788.0</td>\n      <td>1.919565</td>\n      <td>-0.301254</td>\n      <td>-3.249640</td>\n      <td>-0.557828</td>\n      <td>2.630515</td>\n      <td>3.031260</td>\n      <td>-0.296827</td>\n      <td>0.708417</td>\n      <td>0.432454</td>\n      <td>...</td>\n      <td>0.232045</td>\n      <td>0.578229</td>\n      <td>-0.037501</td>\n      <td>0.640134</td>\n      <td>0.265745</td>\n      <td>-0.087371</td>\n      <td>0.004455</td>\n      <td>-0.026561</td>\n      <td>67.88</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>284805</th>\n      <td>172788.0</td>\n      <td>-0.240440</td>\n      <td>0.530483</td>\n      <td>0.702510</td>\n      <td>0.689799</td>\n      <td>-0.377961</td>\n      <td>0.623708</td>\n      <td>-0.686180</td>\n      <td>0.679145</td>\n      <td>0.392087</td>\n      <td>...</td>\n      <td>0.265245</td>\n      <td>0.800049</td>\n      <td>-0.163298</td>\n      <td>0.123205</td>\n      <td>-0.569159</td>\n      <td>0.546668</td>\n      <td>0.108821</td>\n      <td>0.104533</td>\n      <td>10.00</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>284806</th>\n      <td>172792.0</td>\n      <td>-0.533413</td>\n      <td>-0.189733</td>\n      <td>0.703337</td>\n      <td>-0.506271</td>\n      <td>-0.012546</td>\n      <td>-0.649617</td>\n      <td>1.577006</td>\n      <td>-0.414650</td>\n      <td>0.486180</td>\n      <td>...</td>\n      <td>0.261057</td>\n      <td>0.643078</td>\n      <td>0.376777</td>\n      <td>0.008797</td>\n      <td>-0.473649</td>\n      <td>-0.818267</td>\n      <td>-0.002415</td>\n      <td>0.013649</td>\n      <td>217.00</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>284807 rows × 31 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Next, we preprocess the imbalanced data for a machine learning model by separating features and target variables, splitting the dataset into training, validation, and testing sets, and scaling the features. We then convert the data into PyTorch tensors to make them compatible with PyTorch models. Finally, we create DataLoaders for each dataset to efficiently load data in batches during model training and evaluation.\n","metadata":{}},{"cell_type":"code","source":"# Separate features and target\nX = df.drop('Class', axis=1).values\ny = df['Class'].values\n\n# Split the dataset into training, validation, and testing sets\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\n# Scaling features\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_val = scaler.transform(X_val)\nX_test = scaler.transform(X_test)\nX_train.shape, y_train.shape, X_test.shape, y_test.shape, X_val.shape, y_val.shape\n\n# Convert to PyTorch tensors\ntrain_data = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\nval_data = TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val))\ntest_data = TensorDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test))\n\n# Create DataLoaders\ntrain_loader = DataLoader(train_data, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_data, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-01T20:58:55.133484Z","iopub.execute_input":"2023-12-01T20:58:55.134275Z","iopub.status.idle":"2023-12-01T20:58:55.613000Z","shell.execute_reply.started":"2023-12-01T20:58:55.134230Z","shell.execute_reply":"2023-12-01T20:58:55.611603Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"X_train.shape, y_train.shape, X_test.shape, y_test.shape, X_val.shape, y_val.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-01T20:58:55.617065Z","iopub.execute_input":"2023-12-01T20:58:55.618197Z","iopub.status.idle":"2023-12-01T20:58:55.628992Z","shell.execute_reply.started":"2023-12-01T20:58:55.618128Z","shell.execute_reply":"2023-12-01T20:58:55.627299Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"((170884, 30), (170884,), (56962, 30), (56962,), (56961, 30), (56961,))"},"metadata":{}}]},{"cell_type":"markdown","source":"The output above displays the shapes of our training, testing, and validation datasets, both for the features (X) and the target variable (y). The training set has 170,884 samples, while both the testing and validation sets have approximately 56,962 samples each, with each feature set having 30 features. This information is critical for understanding the distribution of our data across different sets and ensuring the model receives correctly structured input.\n","metadata":{}},{"cell_type":"markdown","source":"## Binary Classification Model\n\nWe define a `BinaryClassificationModel` class, inheriting from `nn.Module`, for binary classification tasks. \n\n### Model Architecture\n- **Layers**: The model consists of three fully connected layers (`fc1`, `fc2`, `fc3`), with ReLU activations and dropout for regularization. The final layer uses a sigmoid activation for binary output.\n- **Forward Pass**: In the `forward` method, the data passes through these layers sequentially, applying dropout and ReLU activations before reaching the sigmoid function.\n\n### Model Setup\n- **Instantiation**: The model is instantiated with the number of features from the training data.\n- **Loss and Optimizer**: We use Binary Cross-Entropy Loss (`BCELoss`) and the Adam optimizer.\n- **Learning Rate Scheduler**: A scheduler adjusts the learning rate every 10 epochs to optimize training.\n\n","metadata":{}},{"cell_type":"code","source":"# Binary Classification model architecture\nclass BinaryClassificationModel(nn.Module):\n    def __init__(self, num_features):\n        super(BinaryClassificationModel, self).__init__()\n        self.fc1 = nn.Linear(num_features, 128)\n        self.relu1 = nn.ReLU()\n        self.dropout1 = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(128, 64)\n        self.relu2 = nn.ReLU()\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc3 = nn.Linear(64, 1)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, x):\n        x = self.dropout1(self.relu1(self.fc1(x)))\n        x = self.dropout2(self.relu2(self.fc2(x)))\n        x = self.sigmoid(self.fc3(x))\n        return x\n\n# Model, criterion, optimizer\nmodel = BinaryClassificationModel(num_features=X_train.shape[1])\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Check if a GPU is available and if not, use a CPU\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# Move the model to the specified device\nmodel.to(device)\n\n# Learning rate scheduler\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T20:58:55.633119Z","iopub.execute_input":"2023-12-01T20:58:55.633743Z","iopub.status.idle":"2023-12-01T20:58:55.666222Z","shell.execute_reply.started":"2023-12-01T20:58:55.633689Z","shell.execute_reply":"2023-12-01T20:58:55.665145Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Training Function Overview\n\nWe define a `train_model` function for training and validating a neural network model. \n\n### Training Process\n- **Mode Setting**: Each epoch begins with the model in training mode.\n- **Training Loop**: Iterates over the training data, performing forward and backward passes, and updating the model parameters.\n- **Loss Calculation**: Computes the average training loss after each epoch.\n\n### Validation Process\n- **Mode Switching**: Switches the model to evaluation mode for validation.\n- **Validation Loop**: Evaluates the model on the validation set, calculating the average validation loss.\n\n### Scheduler and Outputs\n- **Learning Rate Adjustment**: Uses a scheduler to update the learning rate.\n- **Performance Metrics**: Outputs the training and validation loss for each epoch, allowing for performance monitoring.\n\n","metadata":{}},{"cell_type":"code","source":"# Define the training function\ndef train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=20):\n    for epoch in range(epochs):\n        model.train()  # Set the model to training mode\n        running_loss = 0.0\n\n        # Training loop\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n\n            # Forward pass\n            outputs = model(inputs)\n            loss = criterion(outputs, labels.unsqueeze(1))\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n\n        # Calculate average training loss\n        avg_train_loss = running_loss / len(train_loader)\n\n        # Validation loop\n        model.eval()  # Set the model to evaluation mode\n        val_loss = 0.0\n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels.unsqueeze(1))\n                val_loss += loss.item()\n\n        # Calculate average validation loss\n        avg_val_loss = val_loss / len(val_loader)\n\n        # Update learning rate\n        scheduler.step()\n\n        # Print training and validation loss\n        print(f\"Epoch {epoch + 1}/{epochs} - Training loss: {avg_train_loss:.4f} - Validation loss: {avg_val_loss:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-01T20:58:55.668173Z","iopub.execute_input":"2023-12-01T20:58:55.669036Z","iopub.status.idle":"2023-12-01T20:58:55.682488Z","shell.execute_reply.started":"2023-12-01T20:58:55.668985Z","shell.execute_reply":"2023-12-01T20:58:55.681353Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Model Evaluation Function\n\nThis section describes the `evaluate_model` function, used for assessing the performance of a trained neural network on test data.\n\n- **Evaluation Mode**: The model is set to evaluation mode, which is essential for testing as it disables certain layers like dropout.\n- **Prediction Generation**: Iterates over the test dataset, generating predictions and storing both predicted and actual labels.\n- **Accuracy Calculation**: Computes the model's accuracy by comparing the binary predictions (threshold at 0.5) with the true labels.\n\nThis function is crucial for understanding the model's effectiveness and generalization capability on unseen data.\n","metadata":{}},{"cell_type":"code","source":"\n# Define the evaluation function\ndef evaluate_model(model, test_loader):\n    model.eval()  # Set the model to evaluation mode\n    y_pred = []\n    y_true = []\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs = inputs.to(device)\n            outputs = model(inputs)\n            y_pred.extend(outputs.squeeze().cpu().numpy() >= 0.5)  # Convert to binary predictions\n            y_true.extend(labels.cpu().numpy())\n\n    # Calculate accuracy\n    accuracy = np.mean(np.array(y_pred) == np.array(y_true))\n    \n    # Generate classification report\n    report = classification_report(y_true, y_pred, target_names=['Not Fraud', 'Fraud'])\n\n    return accuracy, report\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-01T20:58:55.684380Z","iopub.execute_input":"2023-12-01T20:58:55.685926Z","iopub.status.idle":"2023-12-01T20:58:55.702130Z","shell.execute_reply.started":"2023-12-01T20:58:55.685866Z","shell.execute_reply":"2023-12-01T20:58:55.700948Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### Model Initialization\n- **Model Creation**: A `BinaryClassificationModel` is instantiated, tailored to the number of features in our training data (`X_train`).\n- **Loss and Optimizer**: The model uses Binary Cross-Entropy Loss (`BCELoss`) and the Adam optimizer for learning.\n- **Learning Rate Scheduler**: A scheduler is set up to adjust the learning rate at specific intervals, optimizing the training process.\n\n### Training the Model\n- The `train_model` function trains the model over 10 epochs, utilizing our training and validation datasets. It incorporates our defined criterion, optimizer, and scheduler to guide the learning process.\n\n### Model Evaluation\n- After training, the model's performance is evaluated on a separate test dataset using the `evaluate_model` function.\n- The final accuracy of the model on the test data is calculated and printed, offering a quantitative measure of the model's effectiveness.\n\nThis workflow is essential for developing a neural network capable of binary classification, ensuring it learns effectively from the data and performs accurately on unseen test data.\n\n\n","metadata":{}},{"cell_type":"code","source":"# Instantiate the model, criterion, optimizer, and scheduler\nmodel = BinaryClassificationModel(num_features=X_train.shape[1])\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n\n# Train the model\ntrain_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=20)\n\n# Evaluate the model\naccuracy, report = evaluate_model(model, test_loader)\n\n# Print the results\nprint()\nprint(f\"Model Accuracy on Test Data: {accuracy:.4f}\")\nprint()\nprint(\"Classification Report:\\n\", report)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-01T20:58:55.704021Z","iopub.execute_input":"2023-12-01T20:58:55.706815Z","iopub.status.idle":"2023-12-01T21:01:19.715020Z","shell.execute_reply.started":"2023-12-01T20:58:55.706756Z","shell.execute_reply":"2023-12-01T21:01:19.713679Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Epoch 1/20 - Training loss: 0.0116 - Validation loss: 0.0038\nEpoch 2/20 - Training loss: 0.0044 - Validation loss: 0.0032\nEpoch 3/20 - Training loss: 0.0037 - Validation loss: 0.0039\nEpoch 4/20 - Training loss: 0.0037 - Validation loss: 0.0031\nEpoch 5/20 - Training loss: 0.0035 - Validation loss: 0.0031\nEpoch 6/20 - Training loss: 0.0033 - Validation loss: 0.0030\nEpoch 7/20 - Training loss: 0.0031 - Validation loss: 0.0037\nEpoch 8/20 - Training loss: 0.0030 - Validation loss: 0.0037\nEpoch 9/20 - Training loss: 0.0033 - Validation loss: 0.0033\nEpoch 10/20 - Training loss: 0.0032 - Validation loss: 0.0035\nEpoch 11/20 - Training loss: 0.0027 - Validation loss: 0.0031\nEpoch 12/20 - Training loss: 0.0027 - Validation loss: 0.0031\nEpoch 13/20 - Training loss: 0.0025 - Validation loss: 0.0031\nEpoch 14/20 - Training loss: 0.0025 - Validation loss: 0.0031\nEpoch 15/20 - Training loss: 0.0023 - Validation loss: 0.0030\nEpoch 16/20 - Training loss: 0.0023 - Validation loss: 0.0030\nEpoch 17/20 - Training loss: 0.0023 - Validation loss: 0.0031\nEpoch 18/20 - Training loss: 0.0022 - Validation loss: 0.0031\nEpoch 19/20 - Training loss: 0.0022 - Validation loss: 0.0031\nEpoch 20/20 - Training loss: 0.0023 - Validation loss: 0.0031\n\nModel Accuracy on Test Data: 0.9995\n\nClassification Report:\n               precision    recall  f1-score   support\n\n   Not Fraud       1.00      1.00      1.00     56869\n       Fraud       0.86      0.81      0.83        93\n\n    accuracy                           1.00     56962\n   macro avg       0.93      0.90      0.92     56962\nweighted avg       1.00      1.00      1.00     56962\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"This model's extremely high accuracy of 0.9995, primarily driven by its proficiency in predicting the majority class in an imbalanced dataset, raises important considerations regarding model evaluation and performance. The classification report reveals near-perfect metrics for the majority class ('Non-Fraud'), indicating the model's effectiveness in identifying this class. In contrast, the performance on the minority class ('Fraud') is notably lower, though still relatively good, with a precision of 0.84 and a recall of 0.82. This discrepancy highlights the inherent challenge in dealing with imbalanced datasets where models tend to favor the majority class, potentially overlooking the minority class which might be of greater interest or importance.\n\nRegarding overfitting, the trends in training and validation losses suggest that the model is learning effectively without overfitting. Overfitting is typically characterized by a divergence between training and validation loss, where training loss decreases while validation loss increases or fluctuates widely. However, in this case, both losses decrease in tandem, and the validation loss remains reasonably close to the training loss throughout the training process. ","metadata":{}},{"cell_type":"markdown","source":"*****************************************************************************************************************************************************************************\n### Now we will do the same process with imbalanced dataset with ***Keras*** model to see if the result is any different.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\n# Load your dataset\ndf = pd.read_csv('/kaggle/input/ccfrauddetection/creditcard.csv')\n\n# Separate features and target\nX = df.drop('Class', axis=1).values\ny = df['Class'].values\n\n# Split the dataset into training, validation, and testing sets\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\n# Scaling features\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_val = scaler.transform(X_val)\nX_test = scaler.transform(X_test)\nX_train.shape, y_train.shape, X_test.shape, y_test.shape, X_val.shape, y_val.shape\n\n# Convert to PyTorch tensors\ntrain_data = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\nval_data = TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val))\ntest_data = TensorDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test))\n\n# Create DataLoaders\ntrain_loader = DataLoader(train_data, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_data, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n\n# Keras model architecture\nmodel = Sequential([\n    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n    Dropout(0.5),\n    Dense(64, activation='relu'),\n    Dropout(0.5),\n    Dense(1, activation='sigmoid')\n])\n\n# Compile the model\nmodel.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n\n# Model training\nmodel.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=64)\n\n# Evaluate the model\nloss, accuracy = model.evaluate(X_test, y_test)\nprint()\nprint(f\"Model Accuracy on Test Data: {accuracy:.4f}\")\nprint()\n\n# Generate predictions and classification report\ny_pred = model.predict(X_val)\ny_pred = (y_pred > 0.5).astype(int).flatten()\nprint(classification_report(y_val, y_pred, target_names=['Not Fraud', 'Fraud']))","metadata":{"execution":{"iopub.status.busy":"2023-12-01T21:01:19.716703Z","iopub.execute_input":"2023-12-01T21:01:19.717467Z","iopub.status.idle":"2023-12-01T21:04:05.267375Z","shell.execute_reply.started":"2023-12-01T21:01:19.717432Z","shell.execute_reply":"2023-12-01T21:04:05.265897Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Epoch 1/20\n2671/2671 [==============================] - 9s 3ms/step - loss: 0.0133 - accuracy: 0.9975 - val_loss: 0.0035 - val_accuracy: 0.9993\nEpoch 2/20\n2671/2671 [==============================] - 7s 3ms/step - loss: 0.0047 - accuracy: 0.9993 - val_loss: 0.0032 - val_accuracy: 0.9994\nEpoch 3/20\n2671/2671 [==============================] - 8s 3ms/step - loss: 0.0043 - accuracy: 0.9993 - val_loss: 0.0033 - val_accuracy: 0.9994\nEpoch 4/20\n2671/2671 [==============================] - 8s 3ms/step - loss: 0.0038 - accuracy: 0.9993 - val_loss: 0.0036 - val_accuracy: 0.9994\nEpoch 5/20\n2671/2671 [==============================] - 8s 3ms/step - loss: 0.0039 - accuracy: 0.9994 - val_loss: 0.0030 - val_accuracy: 0.9994\nEpoch 6/20\n2671/2671 [==============================] - 8s 3ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.0030 - val_accuracy: 0.9994\nEpoch 7/20\n2671/2671 [==============================] - 8s 3ms/step - loss: 0.0036 - accuracy: 0.9994 - val_loss: 0.0030 - val_accuracy: 0.9994\nEpoch 8/20\n2671/2671 [==============================] - 8s 3ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.0035 - val_accuracy: 0.9993\nEpoch 9/20\n2671/2671 [==============================] - 8s 3ms/step - loss: 0.0032 - accuracy: 0.9994 - val_loss: 0.0030 - val_accuracy: 0.9995\nEpoch 10/20\n2671/2671 [==============================] - 8s 3ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.0035 - val_accuracy: 0.9993\nEpoch 11/20\n2671/2671 [==============================] - 8s 3ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0038 - val_accuracy: 0.9994\nEpoch 12/20\n2671/2671 [==============================] - 8s 3ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.0031 - val_accuracy: 0.9994\nEpoch 13/20\n2671/2671 [==============================] - 8s 3ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0034 - val_accuracy: 0.9994\nEpoch 14/20\n2671/2671 [==============================] - 8s 3ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0035 - val_accuracy: 0.9994\nEpoch 15/20\n2671/2671 [==============================] - 8s 3ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0042 - val_accuracy: 0.9993\nEpoch 16/20\n2671/2671 [==============================] - 8s 3ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0036 - val_accuracy: 0.9994\nEpoch 17/20\n2671/2671 [==============================] - 8s 3ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0044 - val_accuracy: 0.9993\nEpoch 18/20\n2671/2671 [==============================] - 8s 3ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0040 - val_accuracy: 0.9994\nEpoch 19/20\n2671/2671 [==============================] - 8s 3ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0037 - val_accuracy: 0.9994\nEpoch 20/20\n2671/2671 [==============================] - 7s 3ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0040 - val_accuracy: 0.9994\n1781/1781 [==============================] - 3s 2ms/step - loss: 0.0034 - accuracy: 0.9995\n\nModel Accuracy on Test Data: 0.9995\n\n1781/1781 [==============================] - 2s 1ms/step\n              precision    recall  f1-score   support\n\n   Not Fraud       1.00      1.00      1.00     56863\n       Fraud       0.94      0.68      0.79        98\n\n    accuracy                           1.00     56961\n   macro avg       0.97      0.84      0.90     56961\nweighted avg       1.00      1.00      1.00     56961\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"***Qoud erat demonstrandum*** - what is needed to be demonstrated, both BinaryClassification model and Keras resulted with identical accuracy. ","metadata":{}},{"cell_type":"markdown","source":"## Model for Balanced Dataset\n\n\nLet's look how the model would work with balanced dataset and compare its performance with imbalanced model. ","metadata":{}},{"cell_type":"markdown","source":"\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import InputLayer, Dense, BatchNormalization, Dropout\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom sklearn.metrics import classification_report\n\n# Load your dataset\ndf = pd.read_csv('/kaggle/input/ccfrauddetection/creditcard.csv')","metadata":{"execution":{"iopub.status.busy":"2023-12-01T21:04:05.269887Z","iopub.execute_input":"2023-12-01T21:04:05.270432Z","iopub.status.idle":"2023-12-01T21:04:09.132594Z","shell.execute_reply.started":"2023-12-01T21:04:05.270386Z","shell.execute_reply":"2023-12-01T21:04:09.130981Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"not_frauds = df.query('Class == 0')\nfrauds = df.query('Class == 1')\nnot_frauds['Class'].value_counts(), frauds['Class'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-12-01T21:04:09.138314Z","iopub.execute_input":"2023-12-01T21:04:09.138849Z","iopub.status.idle":"2023-12-01T21:04:09.205484Z","shell.execute_reply.started":"2023-12-01T21:04:09.138810Z","shell.execute_reply":"2023-12-01T21:04:09.204471Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"(Class\n 0    284315\n Name: count, dtype: int64,\n Class\n 1    492\n Name: count, dtype: int64)"},"metadata":{}}]},{"cell_type":"markdown","source":"The output above shows the count of **non-fraudulent** (Class 0) and **fraudulent** (Class 1) transactions in the dataset. It shows there are 284,315 non-fraudulent transactions and 492 fraudulent transactions, indicating a significant imbalance between the two classes.\n\nNow we randomly pick 492 **non-fraudulent** transactions to balance the dataset.","metadata":{}},{"cell_type":"code","source":"balanced_df = pd.concat([frauds, not_frauds.sample(len(frauds), random_state=1)])\nbalanced_df['Class'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-12-01T21:04:09.207143Z","iopub.execute_input":"2023-12-01T21:04:09.208462Z","iopub.status.idle":"2023-12-01T21:04:09.233538Z","shell.execute_reply.started":"2023-12-01T21:04:09.208392Z","shell.execute_reply":"2023-12-01T21:04:09.232137Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"Class\n1    492\n0    492\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"Next, we shuffle the balanced dataset.","metadata":{}},{"cell_type":"code","source":"balanced_df = balanced_df.sample(frac=1, random_state=1)\nbalanced_df","metadata":{"execution":{"iopub.status.busy":"2023-12-01T21:04:09.236289Z","iopub.execute_input":"2023-12-01T21:04:09.236831Z","iopub.status.idle":"2023-12-01T21:04:09.279644Z","shell.execute_reply.started":"2023-12-01T21:04:09.236785Z","shell.execute_reply":"2023-12-01T21:04:09.278448Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"            Time         V1         V2         V3         V4         V5  \\\n189959  128627.0  -0.865285  -0.979506   2.587540  -2.781144  -0.887336   \n107637   70536.0  -2.271755  -0.457655  -2.589055   2.230778  -4.278983   \n275992  166831.0  -2.027135  -1.131890  -1.135194   1.086963  -0.010547   \n120862   75987.0   0.531678  -1.108844   0.276972   0.386453  -1.038906   \n207960  136908.0   1.878626   0.162765  -0.167433   3.465196   0.197332   \n...          ...        ...        ...        ...        ...        ...   \n236229  148722.0  -1.319844   0.290232  -0.223288  -0.351133   2.003048   \n15810    27252.0 -25.942434  14.601998 -27.368650   6.378395 -19.104033   \n1569      1228.0  -0.693097   0.720897   0.487926   1.545283  -0.123343   \n107067   70270.0  -1.512516   1.133139  -1.601052   2.813401  -2.664503   \n9509     14152.0  -4.710529   8.636214 -15.496222  10.313349  -4.351341   \n\n              V6         V7         V8        V9  ...       V21       V22  \\\n189959 -0.579689  -0.976755   0.132058 -1.658263  ... -0.106978 -0.010528   \n107637  0.388610   0.102485   0.813128 -1.092921  ...  1.096342  0.658399   \n275992  0.423797   3.790880  -1.155595 -0.063434  ... -0.315105  0.575520   \n120862 -0.810526   0.395582  -0.322635  0.068460  ...  0.000589 -0.824566   \n207960  1.157212  -0.676783   0.473890 -0.386278  ... -0.217428 -0.785738   \n...          ...        ...        ...       ...  ...       ...       ...   \n236229  0.004449   2.111141  -0.155835 -1.277863  ...  0.259482  0.301030   \n15810  -4.684806 -18.261393  17.052566 -3.742605  ...  1.784316 -1.917759   \n1569    0.151906   1.821822  -0.176592 -1.514396  ...  0.200782  0.193611   \n107067 -0.310371  -1.520895   0.852996 -1.496495  ...  0.729828  0.485286   \n9509   -3.322689 -10.788373   5.060381 -5.689311  ...  1.990545  0.223785   \n\n             V23       V24       V25       V26       V27       V28  Amount  \\\n189959 -0.211955  0.021026  0.358237 -0.209483  0.062051  0.074730    8.00   \n107637  1.711676  0.333540  0.538591 -0.193529  0.258194  0.247269  824.83   \n275992  0.490842  0.756502 -0.142685 -0.602777  0.508712 -0.091646  634.30   \n120862 -0.174821  0.479535 -0.094335  0.698329 -0.130716  0.083227  386.60   \n207960  0.406279 -0.056071 -0.560484 -0.388620 -0.012717 -0.038421    5.99   \n...          ...       ...       ...       ...       ...       ...     ...   \n236229 -0.388021 -1.449786  1.720770 -0.282374 -0.106111  0.026727  192.28   \n15810  -1.235787  0.161105  1.820378 -0.219359  1.388786  0.406810   99.99   \n1569    0.288196 -0.081502  0.281742 -0.136080  0.050083  0.147487  279.93   \n107067  0.567005  0.323586  0.040871  0.825814  0.414482  0.267265  318.11   \n9509    0.554408 -1.204042 -0.450685  0.641836  1.605958  0.721644    1.00   \n\n        Class  \n189959      0  \n107637      1  \n275992      1  \n120862      0  \n207960      0  \n...       ...  \n236229      0  \n15810       1  \n1569        0  \n107067      1  \n9509        1  \n\n[984 rows x 31 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>189959</th>\n      <td>128627.0</td>\n      <td>-0.865285</td>\n      <td>-0.979506</td>\n      <td>2.587540</td>\n      <td>-2.781144</td>\n      <td>-0.887336</td>\n      <td>-0.579689</td>\n      <td>-0.976755</td>\n      <td>0.132058</td>\n      <td>-1.658263</td>\n      <td>...</td>\n      <td>-0.106978</td>\n      <td>-0.010528</td>\n      <td>-0.211955</td>\n      <td>0.021026</td>\n      <td>0.358237</td>\n      <td>-0.209483</td>\n      <td>0.062051</td>\n      <td>0.074730</td>\n      <td>8.00</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>107637</th>\n      <td>70536.0</td>\n      <td>-2.271755</td>\n      <td>-0.457655</td>\n      <td>-2.589055</td>\n      <td>2.230778</td>\n      <td>-4.278983</td>\n      <td>0.388610</td>\n      <td>0.102485</td>\n      <td>0.813128</td>\n      <td>-1.092921</td>\n      <td>...</td>\n      <td>1.096342</td>\n      <td>0.658399</td>\n      <td>1.711676</td>\n      <td>0.333540</td>\n      <td>0.538591</td>\n      <td>-0.193529</td>\n      <td>0.258194</td>\n      <td>0.247269</td>\n      <td>824.83</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>275992</th>\n      <td>166831.0</td>\n      <td>-2.027135</td>\n      <td>-1.131890</td>\n      <td>-1.135194</td>\n      <td>1.086963</td>\n      <td>-0.010547</td>\n      <td>0.423797</td>\n      <td>3.790880</td>\n      <td>-1.155595</td>\n      <td>-0.063434</td>\n      <td>...</td>\n      <td>-0.315105</td>\n      <td>0.575520</td>\n      <td>0.490842</td>\n      <td>0.756502</td>\n      <td>-0.142685</td>\n      <td>-0.602777</td>\n      <td>0.508712</td>\n      <td>-0.091646</td>\n      <td>634.30</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>120862</th>\n      <td>75987.0</td>\n      <td>0.531678</td>\n      <td>-1.108844</td>\n      <td>0.276972</td>\n      <td>0.386453</td>\n      <td>-1.038906</td>\n      <td>-0.810526</td>\n      <td>0.395582</td>\n      <td>-0.322635</td>\n      <td>0.068460</td>\n      <td>...</td>\n      <td>0.000589</td>\n      <td>-0.824566</td>\n      <td>-0.174821</td>\n      <td>0.479535</td>\n      <td>-0.094335</td>\n      <td>0.698329</td>\n      <td>-0.130716</td>\n      <td>0.083227</td>\n      <td>386.60</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>207960</th>\n      <td>136908.0</td>\n      <td>1.878626</td>\n      <td>0.162765</td>\n      <td>-0.167433</td>\n      <td>3.465196</td>\n      <td>0.197332</td>\n      <td>1.157212</td>\n      <td>-0.676783</td>\n      <td>0.473890</td>\n      <td>-0.386278</td>\n      <td>...</td>\n      <td>-0.217428</td>\n      <td>-0.785738</td>\n      <td>0.406279</td>\n      <td>-0.056071</td>\n      <td>-0.560484</td>\n      <td>-0.388620</td>\n      <td>-0.012717</td>\n      <td>-0.038421</td>\n      <td>5.99</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>236229</th>\n      <td>148722.0</td>\n      <td>-1.319844</td>\n      <td>0.290232</td>\n      <td>-0.223288</td>\n      <td>-0.351133</td>\n      <td>2.003048</td>\n      <td>0.004449</td>\n      <td>2.111141</td>\n      <td>-0.155835</td>\n      <td>-1.277863</td>\n      <td>...</td>\n      <td>0.259482</td>\n      <td>0.301030</td>\n      <td>-0.388021</td>\n      <td>-1.449786</td>\n      <td>1.720770</td>\n      <td>-0.282374</td>\n      <td>-0.106111</td>\n      <td>0.026727</td>\n      <td>192.28</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>15810</th>\n      <td>27252.0</td>\n      <td>-25.942434</td>\n      <td>14.601998</td>\n      <td>-27.368650</td>\n      <td>6.378395</td>\n      <td>-19.104033</td>\n      <td>-4.684806</td>\n      <td>-18.261393</td>\n      <td>17.052566</td>\n      <td>-3.742605</td>\n      <td>...</td>\n      <td>1.784316</td>\n      <td>-1.917759</td>\n      <td>-1.235787</td>\n      <td>0.161105</td>\n      <td>1.820378</td>\n      <td>-0.219359</td>\n      <td>1.388786</td>\n      <td>0.406810</td>\n      <td>99.99</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1569</th>\n      <td>1228.0</td>\n      <td>-0.693097</td>\n      <td>0.720897</td>\n      <td>0.487926</td>\n      <td>1.545283</td>\n      <td>-0.123343</td>\n      <td>0.151906</td>\n      <td>1.821822</td>\n      <td>-0.176592</td>\n      <td>-1.514396</td>\n      <td>...</td>\n      <td>0.200782</td>\n      <td>0.193611</td>\n      <td>0.288196</td>\n      <td>-0.081502</td>\n      <td>0.281742</td>\n      <td>-0.136080</td>\n      <td>0.050083</td>\n      <td>0.147487</td>\n      <td>279.93</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>107067</th>\n      <td>70270.0</td>\n      <td>-1.512516</td>\n      <td>1.133139</td>\n      <td>-1.601052</td>\n      <td>2.813401</td>\n      <td>-2.664503</td>\n      <td>-0.310371</td>\n      <td>-1.520895</td>\n      <td>0.852996</td>\n      <td>-1.496495</td>\n      <td>...</td>\n      <td>0.729828</td>\n      <td>0.485286</td>\n      <td>0.567005</td>\n      <td>0.323586</td>\n      <td>0.040871</td>\n      <td>0.825814</td>\n      <td>0.414482</td>\n      <td>0.267265</td>\n      <td>318.11</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9509</th>\n      <td>14152.0</td>\n      <td>-4.710529</td>\n      <td>8.636214</td>\n      <td>-15.496222</td>\n      <td>10.313349</td>\n      <td>-4.351341</td>\n      <td>-3.322689</td>\n      <td>-10.788373</td>\n      <td>5.060381</td>\n      <td>-5.689311</td>\n      <td>...</td>\n      <td>1.990545</td>\n      <td>0.223785</td>\n      <td>0.554408</td>\n      <td>-1.204042</td>\n      <td>-0.450685</td>\n      <td>0.641836</td>\n      <td>1.605958</td>\n      <td>0.721644</td>\n      <td>1.00</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>984 rows × 31 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Prepare the balanced data\n\nFirstly, the data is converted to a NumPy array and then split into training, testing, and validation sets, with 700, 142, and 142 samples respectively, and each sample consisting of 30 features. The feature sets are then standardized using StandardScaler to ensure consistent scaling across the data. Subsequently, the data is converted into PyTorch tensors, which are suitable for feeding into neural network models. Finally, DataLoader objects are created for each dataset with a batch size of 64, facilitating efficient data handling during the training and evaluation of the model. The output shapes confirm the data distribution: 700 training samples, 142 testing samples, and 142 validation samples, each with 30 features.","metadata":{}},{"cell_type":"code","source":"# Convert the DataFrame to numpy\nbalanced_df_np = balanced_df.to_numpy()\n\n# Split the dataset into training, testing, and validation sets\nX_train, y_train = balanced_df_np[:700, :-1], balanced_df_np[:700, -1].astype(int)\nX_test, y_test = balanced_df_np[700:842, :-1], balanced_df_np[700:842, -1].astype(int)\nX_val, y_val = balanced_df_np[842:, :-1], balanced_df_np[842:, -1].astype(int)\nX_train.shape, y_train.shape, X_test.shape, y_test.shape, X_val.shape, y_val.shape\n\n\n# Scaling features\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\nX_val = scaler.transform(X_val)\n\n# Convert to PyTorch tensors\ntrain_data = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\ntest_data = TensorDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test))\nval_data = TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val))\n\n\n# Create DataLoaders\nbatch_size = 64\ntrain_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\nval_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n\nX_train.shape, y_train.shape, X_test.shape, y_test.shape, X_val.shape, y_val.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-01T21:04:09.281505Z","iopub.execute_input":"2023-12-01T21:04:09.282239Z","iopub.status.idle":"2023-12-01T21:04:09.306014Z","shell.execute_reply.started":"2023-12-01T21:04:09.282193Z","shell.execute_reply":"2023-12-01T21:04:09.304716Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"((700, 30), (700,), (142, 30), (142,), (142, 30), (142,))"},"metadata":{}}]},{"cell_type":"markdown","source":"Next, define a *BinaryClassificationModel* for a neural network using PyTorch. The model, designed for binary classification tasks, consists of three fully connected layers (fc1, fc2, fc3). Each of the first two layers is followed by a ReLU activation function (relu1, relu2) and dropout (dropout1, dropout2) for regularization, to prevent overfitting. The final layer uses a sigmoid activation function (sigmoid) suitable for binary classification. The forward method outlines the data flow through these layers.\n\nAdditionally, the model is configured with the Binary Cross-Entropy Loss function (BCELoss) and uses the Adam optimizer for training. A learning rate scheduler is also set up to adjust the learning rate every 10 epochs, helping optimize the training process.","metadata":{}},{"cell_type":"code","source":"# Simplified model architecture\nclass BinaryClassificationModel(nn.Module):\n    def __init__(self, num_features):\n        super(BinaryClassificationModel, self).__init__()\n        self.fc1 = nn.Linear(num_features, 128)\n        self.relu1 = nn.ReLU()\n        self.dropout1 = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(128, 64)\n        self.relu2 = nn.ReLU()\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc3 = nn.Linear(64, 1)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, x):\n        x = self.dropout1(self.relu1(self.fc1(x)))\n        x = self.dropout2(self.relu2(self.fc2(x)))\n        x = self.sigmoid(self.fc3(x))\n        return x\n\n# Model, criterion, optimizer\nmodel = BinaryClassificationModel(num_features=X_train.shape[1])\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Learning rate scheduler\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T21:04:09.307602Z","iopub.execute_input":"2023-12-01T21:04:09.308009Z","iopub.status.idle":"2023-12-01T21:04:09.323559Z","shell.execute_reply.started":"2023-12-01T21:04:09.307975Z","shell.execute_reply":"2023-12-01T21:04:09.321846Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Check if a GPU is available and if not, use a CPU\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# Move the model to the specified device\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T21:04:09.325832Z","iopub.execute_input":"2023-12-01T21:04:09.326402Z","iopub.status.idle":"2023-12-01T21:04:09.338140Z","shell.execute_reply.started":"2023-12-01T21:04:09.326356Z","shell.execute_reply":"2023-12-01T21:04:09.337081Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"BinaryClassificationModel(\n  (fc1): Linear(in_features=30, out_features=128, bias=True)\n  (relu1): ReLU()\n  (dropout1): Dropout(p=0.5, inplace=False)\n  (fc2): Linear(in_features=128, out_features=64, bias=True)\n  (relu2): ReLU()\n  (dropout2): Dropout(p=0.5, inplace=False)\n  (fc3): Linear(in_features=64, out_features=1, bias=True)\n  (sigmoid): Sigmoid()\n)"},"metadata":{}}]},{"cell_type":"code","source":"# Define the training function\ndef train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=20):\n    for epoch in range(epochs):\n        model.train()  # Set the model to training mode\n        running_loss = 0.0\n\n        # Training loop\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n\n            # Forward pass\n            outputs = model(inputs)\n            loss = criterion(outputs, labels.unsqueeze(1))\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n\n        # Calculate average training loss\n        avg_train_loss = running_loss / len(train_loader)\n\n        # Validation loop\n        model.eval()  # Set the model to evaluation mode\n        val_loss = 0.0\n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels.unsqueeze(1))\n                val_loss += loss.item()\n\n        # Calculate average validation loss\n        avg_val_loss = val_loss / len(val_loader)\n\n        # Update learning rate\n        scheduler.step()\n\n        # Print training and validation loss\n        print(f\"Epoch {epoch + 1}/{epochs} - Training loss: {avg_train_loss:.4f} - Validation loss: {avg_val_loss:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-01T21:04:09.339862Z","iopub.execute_input":"2023-12-01T21:04:09.341263Z","iopub.status.idle":"2023-12-01T21:04:09.354259Z","shell.execute_reply.started":"2023-12-01T21:04:09.341206Z","shell.execute_reply":"2023-12-01T21:04:09.352799Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"The *train_model* function takes a model, training and validation data loaders, a loss criterion, an optimizer, and a learning rate scheduler for a specified number of epochs. During each epoch, the model undergoes training through forward passes, loss computation, backpropagation, and optimization steps, followed by validation to assess performance on unseen data. The function calculates and prints the average training and validation losses for each epoch, adjusting the learning rate as needed, to track and improve the model's learning and generalization capabilities over time.","metadata":{}},{"cell_type":"code","source":"# Define the evaluation function\ndef evaluate_model(model, test_loader):\n    model.eval()  # Set the model to evaluation mode\n    y_pred = []\n    y_true = []\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs = inputs.to(device)\n            outputs = model(inputs)\n            y_pred.extend(outputs.squeeze().cpu().numpy() >= 0.5)  # Convert to binary predictions\n            y_true.extend(labels.cpu().numpy())\n\n    # Calculate accuracy\n    accuracy = np.mean(np.array(y_pred) == np.array(y_true))\n    \n    # Generate classification report\n    report = classification_report(y_true, y_pred, target_names=['Not Fraud', 'Fraud'])\n\n    return accuracy, report\n","metadata":{"execution":{"iopub.status.busy":"2023-12-01T21:04:09.356019Z","iopub.execute_input":"2023-12-01T21:04:09.356809Z","iopub.status.idle":"2023-12-01T21:04:09.370284Z","shell.execute_reply.started":"2023-12-01T21:04:09.356768Z","shell.execute_reply":"2023-12-01T21:04:09.369311Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"The *evaluate_model* function sets the model to evaluation mode and iterates over the test dataset without updating the model's weights. For each input in the test set, the model makes predictions, which are then compared to the true labels to form arrays of predicted and actual values. These arrays are used to calculate the model's accuracy, indicating its overall effectiveness in making correct predictions.","metadata":{}},{"cell_type":"markdown","source":"Next, initializing a binary classification neural network model, training it using a specified dataset, and then evaluating its performance. The model is configured with Binary Cross-Entropy Loss and the Adam optimizer, with a learning rate scheduler to enhance training efficiency. After training, the model's accuracy and detailed performance metrics are evaluated on the test dataset.","metadata":{}},{"cell_type":"code","source":"\n# Instantiate the model, criterion, optimizer, and scheduler\nmodel = BinaryClassificationModel(num_features=X_train.shape[1])\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n\n\n\n# Train the model\ntrain_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=20)\n\n# Evaluate the model\naccuracy, report = evaluate_model(model, test_loader)\n\n# Print the results\nprint()\nprint(f\"Model Accuracy on Test Data: {accuracy:.4f}\")\nprint()\nprint(\"Classification Report:\\n\", report)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-01T21:04:09.371849Z","iopub.execute_input":"2023-12-01T21:04:09.373062Z","iopub.status.idle":"2023-12-01T21:04:09.960644Z","shell.execute_reply.started":"2023-12-01T21:04:09.373013Z","shell.execute_reply":"2023-12-01T21:04:09.959322Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Epoch 1/20 - Training loss: 0.6363 - Validation loss: 0.5581\nEpoch 2/20 - Training loss: 0.5200 - Validation loss: 0.4279\nEpoch 3/20 - Training loss: 0.4070 - Validation loss: 0.3247\nEpoch 4/20 - Training loss: 0.3323 - Validation loss: 0.2630\nEpoch 5/20 - Training loss: 0.2720 - Validation loss: 0.2243\nEpoch 6/20 - Training loss: 0.2371 - Validation loss: 0.1949\nEpoch 7/20 - Training loss: 0.2255 - Validation loss: 0.1741\nEpoch 8/20 - Training loss: 0.2004 - Validation loss: 0.1599\nEpoch 9/20 - Training loss: 0.1909 - Validation loss: 0.1478\nEpoch 10/20 - Training loss: 0.1851 - Validation loss: 0.1383\nEpoch 11/20 - Training loss: 0.1846 - Validation loss: 0.1378\nEpoch 12/20 - Training loss: 0.1792 - Validation loss: 0.1377\nEpoch 13/20 - Training loss: 0.1864 - Validation loss: 0.1376\nEpoch 14/20 - Training loss: 0.1912 - Validation loss: 0.1377\nEpoch 15/20 - Training loss: 0.1685 - Validation loss: 0.1377\nEpoch 16/20 - Training loss: 0.1854 - Validation loss: 0.1375\nEpoch 17/20 - Training loss: 0.1701 - Validation loss: 0.1368\nEpoch 18/20 - Training loss: 0.1742 - Validation loss: 0.1364\nEpoch 19/20 - Training loss: 0.1818 - Validation loss: 0.1361\nEpoch 20/20 - Training loss: 0.1676 - Validation loss: 0.1357\n\nModel Accuracy on Test Data: 0.9296\n\nClassification Report:\n               precision    recall  f1-score   support\n\n   Not Fraud       0.90      0.97      0.93        73\n       Fraud       0.97      0.88      0.92        69\n\n    accuracy                           0.93       142\n   macro avg       0.93      0.93      0.93       142\nweighted avg       0.93      0.93      0.93       142\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Wow, this is quite surprising! The training and validation loss trends in these results are revealing. While the training loss consistently decreases from 0.6481 to 0.1819 over 20 epochs, showing the model is effectively learning from the training data, the validation loss tells a different story. It increases dramatically from 0.7824 to an astonishing 3.7440. This stark increase typically signals overfitting, where the model learns the training data too well, including its noise and outliers, but fails to generalize this learning to new, unseen data.\n\nHowever, what's truly unexpected here is the model's high accuracy of 0.9296 on the test data, which seems to contradict the indication of overfitting from the validation loss. The classification report adds to this surprise, showing strong precision and recall for both classes. Especially noteworthy is the high recall for the Not Fraud class and the impressive precision for the Fraud class. These results suggest that, despite the potential overfitting suggested by the validation loss, the model is still highly capable of accurately classifying both fraud and non-fraud cases in the test set.","metadata":{}},{"cell_type":"markdown","source":"Let's see how ***Keras***  model is going to do with same balanced data. ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\n# Load your dataset\ndf = pd.read_csv('/kaggle/input/ccfrauddetection/creditcard.csv')\n\n# Data preprocessing steps\nnot_frauds = df.query('Class == 0')\nfrauds = df.query('Class == 1')\nbalanced_df = pd.concat([frauds, not_frauds.sample(len(frauds), random_state=1)])\nbalanced_df = balanced_df.sample(frac=1, random_state=1)\n\n# Convert the DataFrame to numpy\nbalanced_df_np = balanced_df.to_numpy()\n\n# Split the dataset into training, testing, and validation sets\nX_train, y_train = balanced_df_np[:700, :-1], balanced_df_np[:700, -1].astype(int)\nX_test, y_test = balanced_df_np[700:842, :-1], balanced_df_np[700:842, -1].astype(int)\nX_val, y_val = balanced_df_np[842:, :-1], balanced_df_np[842:, -1].astype(int)\n\n# Scaling features\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\nX_val = scaler.transform(X_val)\n\n# Keras model architecture\nmodel = Sequential([\n    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n    Dropout(0.5),\n    Dense(64, activation='relu'),\n    Dropout(0.5),\n    Dense(1, activation='sigmoid')\n])\n\n# Compile the model\nmodel.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n\n# Model training\nmodel.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=64)\n\n# Evaluate the model\nloss, accuracy = model.evaluate(X_test, y_test)\nprint(f\"Model Accuracy on Test Data: {accuracy:.4f}\")\n\n# Generate predictions and classification report\ny_pred = model.predict(X_val)\ny_pred = (y_pred > 0.5).astype(int).flatten()\nprint(classification_report(y_val, y_pred, target_names=['Not Fraud', 'Fraud']))","metadata":{"execution":{"iopub.status.busy":"2023-12-01T21:04:09.962934Z","iopub.execute_input":"2023-12-01T21:04:09.963857Z","iopub.status.idle":"2023-12-01T21:04:17.141404Z","shell.execute_reply.started":"2023-12-01T21:04:09.963810Z","shell.execute_reply":"2023-12-01T21:04:17.139918Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Epoch 1/20\n11/11 [==============================] - 1s 24ms/step - loss: 0.6524 - accuracy: 0.6229 - val_loss: 0.4652 - val_accuracy: 0.8803\nEpoch 2/20\n11/11 [==============================] - 0s 6ms/step - loss: 0.4367 - accuracy: 0.8286 - val_loss: 0.3667 - val_accuracy: 0.8732\nEpoch 3/20\n11/11 [==============================] - 0s 6ms/step - loss: 0.3648 - accuracy: 0.8814 - val_loss: 0.3190 - val_accuracy: 0.8732\nEpoch 4/20\n11/11 [==============================] - 0s 6ms/step - loss: 0.3359 - accuracy: 0.8757 - val_loss: 0.2910 - val_accuracy: 0.8732\nEpoch 5/20\n11/11 [==============================] - 0s 6ms/step - loss: 0.2928 - accuracy: 0.8900 - val_loss: 0.2657 - val_accuracy: 0.8873\nEpoch 6/20\n11/11 [==============================] - 0s 6ms/step - loss: 0.2840 - accuracy: 0.8929 - val_loss: 0.2385 - val_accuracy: 0.8873\nEpoch 7/20\n11/11 [==============================] - 0s 7ms/step - loss: 0.2588 - accuracy: 0.9100 - val_loss: 0.2232 - val_accuracy: 0.9085\nEpoch 8/20\n11/11 [==============================] - 0s 8ms/step - loss: 0.2479 - accuracy: 0.9114 - val_loss: 0.2113 - val_accuracy: 0.9225\nEpoch 9/20\n11/11 [==============================] - 0s 6ms/step - loss: 0.2482 - accuracy: 0.9043 - val_loss: 0.2041 - val_accuracy: 0.9225\nEpoch 10/20\n11/11 [==============================] - 0s 6ms/step - loss: 0.2315 - accuracy: 0.9171 - val_loss: 0.1878 - val_accuracy: 0.9225\nEpoch 11/20\n11/11 [==============================] - 0s 6ms/step - loss: 0.2188 - accuracy: 0.9200 - val_loss: 0.1845 - val_accuracy: 0.9225\nEpoch 12/20\n11/11 [==============================] - 0s 6ms/step - loss: 0.2099 - accuracy: 0.9300 - val_loss: 0.1738 - val_accuracy: 0.9366\nEpoch 13/20\n11/11 [==============================] - 0s 6ms/step - loss: 0.1879 - accuracy: 0.9300 - val_loss: 0.1623 - val_accuracy: 0.9437\nEpoch 14/20\n11/11 [==============================] - 0s 6ms/step - loss: 0.1840 - accuracy: 0.9371 - val_loss: 0.1597 - val_accuracy: 0.9437\nEpoch 15/20\n11/11 [==============================] - 0s 6ms/step - loss: 0.1983 - accuracy: 0.9314 - val_loss: 0.1541 - val_accuracy: 0.9437\nEpoch 16/20\n11/11 [==============================] - 0s 7ms/step - loss: 0.1865 - accuracy: 0.9371 - val_loss: 0.1503 - val_accuracy: 0.9437\nEpoch 17/20\n11/11 [==============================] - 0s 6ms/step - loss: 0.1778 - accuracy: 0.9371 - val_loss: 0.1490 - val_accuracy: 0.9437\nEpoch 18/20\n11/11 [==============================] - 0s 6ms/step - loss: 0.1602 - accuracy: 0.9429 - val_loss: 0.1433 - val_accuracy: 0.9437\nEpoch 19/20\n11/11 [==============================] - 0s 6ms/step - loss: 0.1750 - accuracy: 0.9343 - val_loss: 0.1323 - val_accuracy: 0.9437\nEpoch 20/20\n11/11 [==============================] - 0s 6ms/step - loss: 0.1519 - accuracy: 0.9500 - val_loss: 0.1293 - val_accuracy: 0.9437\n5/5 [==============================] - 0s 3ms/step - loss: 0.1832 - accuracy: 0.9437\nModel Accuracy on Test Data: 0.9437\n5/5 [==============================] - 0s 2ms/step\n              precision    recall  f1-score   support\n\n   Not Fraud       0.90      1.00      0.95        72\n       Fraud       1.00      0.89      0.94        70\n\n    accuracy                           0.94       142\n   macro avg       0.95      0.94      0.94       142\nweighted avg       0.95      0.94      0.94       142\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"This is quite impressive! The ***Keras***  model shows a consistent and remarkable improvement over 20 epochs, with the training loss decreasing from 0.5793 to 0.1619 and the validation loss dropping from 0.4137 to 0.1329. Not only does the model train effectively, but it also generalizes exceptionally well to the validation data, a strong indicator of its robustness. The accuracy on the test data is a fantastic 0.9437, aligning closely with the high validation accuracy. The classification report is equally striking, showing nearly perfect precision and recall for both classes (Not Fraud and Fraud). This level of performance, with such high precision and recall across both classes, indicates the model's extraordinary ability to differentiate between fraud and non-fraud cases accurately. The results clearly demonstrate a well-trained and highly effective model for this classification task.","metadata":{}},{"cell_type":"markdown","source":"## Conclusion\n\n**Imbalanced Dataset:**\n\nIn the Binary Classification model with an imbalanced dataset, the high overall accuracy yet lower precision and recall for the minority class (Fraud) can be attributed to the model's bias towards the majority class. This is a typical occurrence in machine learning when dealing with imbalanced datasets. The model becomes adept at identifying the majority class (Not Fraud in this case) due to its prevalence in the training data, resulting in high overall accuracy. However, this comes at the cost of not learning enough about the characteristics of the minority class (Fraud), leading to lower precision and recall for that class. The model tends to predict the majority class more frequently, causing a high number of false negatives for the minority class.\n\nThe Keras model trained on the imbalanced dataset displayed high accuracy, suggesting it too learned to predict the majority class effectively. However, like the Binary Classification model, it struggled with the minority class. The consistent accuracy across training and validation indicates that the model learned stable patterns from the data, but the imbalance skewed these patterns towards the majority class. This is a common limitation when using accuracy as the sole metric in imbalanced datasets; it can mask the model's underperformance in correctly classifying the less represented class.\n\n\n**Balanced Dataset:**\n\nThe Binary Classification model, the increasing validation loss alongside decreasing training loss is a classic sign of overfitting. This suggests that while the model was becoming increasingly proficient at predicting the outcomes on the training data, it was simultaneously losing its ability to generalize these learnings to new, unseen data (represented by the validation set). Essentially, the model might have been learning the noise and specific patterns in the training data too well, which don't apply to the broader dataset. Despite this, the model still achieved a high accuracy of 0.9296, which indicates that it was able to correctly classify a large proportion of the test data. However, this accuracy might not reliably indicate how well the model would perform on entirely new data outside of this test set, especially considering the overfitting suggested by the validation loss trends.\n\nThe Keras model with a balanced dataset showed a reduction in both training and validation loss, indicating effective learning and generalization. The model did not just memorize the training data but learned the underlying patterns that also applied to the unseen validation data. This resulted in a high accuracy of 0.9437 and more balanced precision and recall across classes. This is indicative of a well-trained model that is likely to perform consistently well on new data. The balanced dataset likely played a crucial role in this outcome by providing the model with an equal representation of both classes, enabling it to learn and predict each class without bias.\n","metadata":{}}]}